{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "class LogMelFilterBanks(nn.Module):\n",
        "    def __init__(self, n_fft=400, hop_length=160, n_mels=80, sr=16000, f_min=0.0, f_max=None, mel_scale='htk'):\n",
        "        super().__init__()\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "        self.sr = sr\n",
        "        self.f_min = f_min\n",
        "        self.f_max = f_max if f_max is not None else sr / 2\n",
        "        self.mel_scale = mel_scale\n",
        "\n",
        "        window = torch.hann_window(self.n_fft)\n",
        "        self.register_buffer(\"window\", window, persistent=False)\n",
        "\n",
        "        fbanks = self._init_melscale_fbanks()\n",
        "        self.register_buffer(\"fbanks\", fbanks, persistent=False)\n",
        "\n",
        "    def _init_melscale_fbanks(self):\n",
        "        fbanks = torchaudio.functional.melscale_fbanks(\n",
        "            n_freqs=self.n_fft // 2 + 1,\n",
        "            n_mels=self.n_mels,\n",
        "            sample_rate=self.sr,\n",
        "            f_min=self.f_min,\n",
        "            f_max=self.f_max,\n",
        "            mel_scale=self.mel_scale\n",
        "        )\n",
        "        return fbanks\n",
        "\n",
        "    def spectrogram(self, x):\n",
        "        stft = torch.stft(\n",
        "            x,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            window=self.window,\n",
        "            return_complex=True\n",
        "        )\n",
        "        return stft.abs().pow(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spec = self.spectrogram(x)\n",
        "        mel = torch.matmul(spec.transpose(1, 2), self.fbanks)\n",
        "        mel = mel.transpose(1, 2)\n",
        "        return torch.log(mel + 1e-6)\n"
      ],
      "metadata": {
        "id": "uRKBAC2zrAWU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pA3tiLlMyzE",
        "outputId": "d7ea164d-b12c-4b9b-a529-326e67009ada"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m733.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ptflops\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ptflops-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rye6237aqxZt",
        "outputId": "3630a23a-2457-4c38-9217-b551b169a304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with n_mels=20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Фильтрация образцов yes/no: 100%|██████████| 105829/105829 [06:36<00:00, 266.59it/s]\n",
            "Фильтрация образцов yes/no: 100%|██████████| 9981/9981 [00:32<00:00, 304.31it/s]\n",
            "Фильтрация образцов yes/no: 100%|██████████| 11005/11005 [00:36<00:00, 301.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | TrainLoss=0.1686 | ValAcc=0.9626 | Time=1.70s\n",
            "Epoch 2 | TrainLoss=0.1040 | ValAcc=0.9601 | Time=1.45s\n",
            "Epoch 3 | TrainLoss=0.0841 | ValAcc=0.9726 | Time=1.38s\n",
            "Test Acc: 0.9684\n",
            "Params: 5746\n",
            "FLOPs: 187.2 KMac\n",
            "\n",
            "Training with n_mels=40\n",
            "Epoch 1 | TrainLoss=0.2105 | ValAcc=0.9639 | Time=1.54s\n",
            "Epoch 2 | TrainLoss=0.1033 | ValAcc=0.9763 | Time=1.76s\n",
            "Epoch 3 | TrainLoss=0.0894 | ValAcc=0.9552 | Time=1.45s\n",
            "Test Acc: 0.9672\n",
            "Params: 6706\n",
            "FLOPs: 283.2 KMac\n",
            "\n",
            "Training with n_mels=80\n",
            "Epoch 1 | TrainLoss=0.2131 | ValAcc=0.9589 | Time=1.47s\n",
            "Epoch 2 | TrainLoss=0.1217 | ValAcc=0.9751 | Time=1.46s\n",
            "Epoch 3 | TrainLoss=0.0957 | ValAcc=0.9851 | Time=1.44s\n",
            "Test Acc: 0.9854\n",
            "Params: 8626\n",
            "FLOPs: 475.2 KMac\n",
            "\n",
            "Training with groups=2\n",
            "Epoch 1 | TrainLoss=0.2028 | ValAcc=0.9614 | Time=1.43s\n",
            "Epoch 2 | TrainLoss=0.1183 | ValAcc=0.9601 | Time=1.69s\n",
            "Epoch 3 | TrainLoss=0.1043 | ValAcc=0.9514 | Time=1.53s\n",
            "Test Acc: 0.9709\n",
            "Params: 5938\n",
            "FLOPs: 244.8 KMac\n",
            "\n",
            "Training with groups=4\n",
            "Epoch 1 | TrainLoss=0.2407 | ValAcc=0.9626 | Time=1.43s\n",
            "Epoch 2 | TrainLoss=0.1358 | ValAcc=0.9564 | Time=1.43s\n",
            "Epoch 3 | TrainLoss=0.1123 | ValAcc=0.9714 | Time=1.42s\n",
            "Test Acc: 0.9867\n",
            "Params: 4594\n",
            "FLOPs: 129.6 KMac\n",
            "\n",
            "Training with groups=8\n",
            "Epoch 1 | TrainLoss=0.3012 | ValAcc=0.9315 | Time=1.42s\n",
            "Epoch 2 | TrainLoss=0.1526 | ValAcc=0.9577 | Time=1.54s\n",
            "Epoch 3 | TrainLoss=0.1222 | ValAcc=0.9651 | Time=1.69s\n",
            "Test Acc: 0.9612\n",
            "Params: 3922\n",
            "FLOPs: 72.0 KMac\n",
            "\n",
            "Training with groups=16\n",
            "Epoch 1 | TrainLoss=0.3425 | ValAcc=0.9116 | Time=1.36s\n",
            "Epoch 2 | TrainLoss=0.2201 | ValAcc=0.9303 | Time=1.35s\n",
            "Epoch 3 | TrainLoss=0.1825 | ValAcc=0.9278 | Time=1.37s\n",
            "Test Acc: 0.9515\n",
            "Params: 3586\n",
            "FLOPs: 43.2 KMac\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "\n",
        "# from melbanks import LogMelFilterBanks\n",
        "\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, root, subset=None):\n",
        "        super().__init__(root, download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath, \"r\") as f:\n",
        "                return [os.path.join(self._path, line.strip()) for line in f]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            # Исключаем файлы валидации и тестирования\n",
        "            val_list = set(load_list(\"validation_list.txt\"))\n",
        "            test_list = set(load_list(\"testing_list.txt\"))\n",
        "            excludes = val_list.union(test_list)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "\n",
        "class YesNoDataset(torch.utils.data.Dataset):\n",
        "    _cache = {}\n",
        "\n",
        "    def __init__(self, root, subset, transform=None):\n",
        "        self.dataset = SubsetSC(root, subset=subset)\n",
        "        self.transform = transform\n",
        "        key = f\"{root}_{subset}\"\n",
        "        if key in YesNoDataset._cache:\n",
        "            self.yesno_samples = YesNoDataset._cache[key]\n",
        "        else:\n",
        "            self.yesno_samples = []\n",
        "            for waveform, sr, label, _, _ in tqdm(self.dataset, desc=\"Фильтрация образцов yes/no\"):\n",
        "                if label in [\"yes\", \"no\"]:\n",
        "                    self.yesno_samples.append((waveform, sr, label))\n",
        "            YesNoDataset._cache[key] = self.yesno_samples\n",
        "        self.label_map = {\"no\": 0, \"yes\": 1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.yesno_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sr, label = self.yesno_samples[idx]\n",
        "        if sr != 16000:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        if self.transform is not None:\n",
        "            waveform = self.transform(waveform)\n",
        "        target = self.label_map[label]\n",
        "        return waveform, target\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_mels=80, groups=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_mels, 16, kernel_size=3, padding=1, groups=groups)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1, groups=groups)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.adapt_pool = nn.AdaptiveAvgPool1d(50)\n",
        "        self.fc = nn.Linear(32 * 50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.adapt_pool(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def pad_crop(tensor, fixed_length):\n",
        "    # tensor shape: (n_mels, frames)\n",
        "    if tensor.shape[-1] > fixed_length:\n",
        "        return tensor[:, :fixed_length]\n",
        "    elif tensor.shape[-1] < fixed_length:\n",
        "        return F.pad(tensor, (0, fixed_length - tensor.shape[-1]))\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def train_model(n_mels=80, groups=1, epochs=5, batch_size=32, device=\"cuda\"):\n",
        "    root = \"./SpeechCommands\"\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "\n",
        "    train_ds = YesNoDataset(root, subset=\"training\")\n",
        "    val_ds = YesNoDataset(root, subset=\"validation\")\n",
        "    test_ds = YesNoDataset(root, subset=\"testing\")\n",
        "\n",
        "    log_mel_transform = LogMelFilterBanks(n_mels=n_mels).to(device)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        waveforms, targets = zip(*batch)\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "        max_waveform_length = max([w.shape[-1] for w in waveforms])\n",
        "        padded_waveforms = [F.pad(w, (0, max_waveform_length - w.shape[-1])) for w in waveforms]\n",
        "        waveforms_padded = torch.stack(padded_waveforms).squeeze(1).to(device)\n",
        "        log_mels = log_mel_transform(waveforms_padded)\n",
        "        fixed_length = 100\n",
        "        fixed_log_mels = [pad_crop(mel, fixed_length) for mel in log_mels]\n",
        "        batched_log_mels = torch.stack(fixed_log_mels)\n",
        "        return batched_log_mels, targets.to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = SimpleCNN(n_mels=n_mels, groups=groups).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x_batch)\n",
        "            loss = criterion(out, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * x_batch.size(0)\n",
        "        total_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                preds = model(x_batch).argmax(dim=1)\n",
        "                correct += (preds == y_batch).sum().item()\n",
        "                total += y_batch.size(0)\n",
        "        val_acc = correct / total\n",
        "        epoch_time = time.time() - start\n",
        "        print(f\"Epoch {epoch} | TrainLoss={total_loss:.4f} | ValAcc={val_acc:.4f} | Time={epoch_time:.2f}s\")\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            preds = model(x_batch).argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Acc: {test_acc:.4f}\")\n",
        "    print(f\"Params: {count_params(model)}\")\n",
        "\n",
        "    try:\n",
        "        from ptflops import get_model_complexity_info\n",
        "        macs, _ = get_model_complexity_info(model, (n_mels, 100), as_strings=True, print_per_layer_stat=False)\n",
        "        print(f\"FLOPs: {macs}\")\n",
        "    except ImportError:\n",
        "        print(\"ptflops library not found\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for nm in [20, 40, 80]:\n",
        "        print(f\"\\nTraining with n_mels={nm}\")\n",
        "        train_model(n_mels=nm, groups=1, epochs=3)\n",
        "\n",
        "    for g in [2, 4, 8, 16]:\n",
        "        print(f\"\\nTraining with groups={g}\")\n",
        "        train_model(n_mels=80, groups=g, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q9HqTZRpuAqu"
      }
    }
  ]
}